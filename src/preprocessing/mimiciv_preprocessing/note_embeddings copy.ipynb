{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wang/junh/envs/fuse_env/lib/python3.8/site-packages/dask/dataframe/utils.py:374: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/data/wang/junh/envs/fuse_env/lib/python3.8/site-packages/dask/dataframe/utils.py:374: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/data/wang/junh/envs/fuse_env/lib/python3.8/site-packages/dask/dataframe/utils.py:374: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from MIMIC_IV_HAIM_API import split_note_document, get_biobert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dir = \"/data/wang/junh/datasets/multimodal\"\n",
    "output_dir = os.path.join(mm_dir, \"preprocessing\")\n",
    "\n",
    "notes_df = pd.read_pickle(os.path.join(output_dir, \"clinic_notes_text.pkl\"))\n",
    "rad_notes_df = pd.read_pickle(os.path.join(output_dir, \"notes_text.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
       "       'charttime', 'storetime', 'text', 'stay_id', 'icu_time_delta',\n",
       "       'hosp_time_delta'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df.rename(columns={\"text\": \"dis_text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_notes_df = rad_notes_df.merge(notes_df[['subject_id','hadm_id', 'note_id', 'dis_text']], on=['subject_id','hadm_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_rad_notes_df = rad_notes_df[rad_notes_df['stay_id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ICU radiology notes:  282833\n",
      "Number of unique stays:  56824\n",
      "Missing discharge summaries:  3996\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of ICU radiology notes: \", len(icu_rad_notes_df))\n",
    "print(\"Number of unique stays: \", len(icu_rad_notes_df['stay_id'].unique()))\n",
    "print(\"Missing discharge summaries: \", icu_rad_notes_df['dis_text'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_rad_notes_df.dropna(subset=['dis_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['note_id_x', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
       "       'charttime', 'storetime', 'text', 'stay_id', 'icu_time_delta',\n",
       "       'hosp_time_delta', 'note_id_y', 'dis_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icu_rad_notes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17428/17428 [36:59:38<00:00,  7.64s/it]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Set batch size (you can tune this based on your GPU memory)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Set device to use GPU with ID 1\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "icu_rad_notes_df['biobert_embeddings'] = None\n",
    "icu_rad_notes_df['longformer_embeddings'] = None\n",
    "\n",
    "def process_batch(chunk_batch, device, model_name='biobert'):\n",
    "    # Process all chunks in the batch at once\n",
    "    curr_embeddings, _ = get_biobert_embeddings(chunk_batch, device, model_name)\n",
    "    embeddings_batch = curr_embeddings.detach().cpu().numpy()\n",
    "    return embeddings_batch\n",
    "\n",
    "# Process in batches\n",
    "for index_start in tqdm(range(0, icu_rad_notes_df.shape[0], BATCH_SIZE)):\n",
    "    index_end = min(index_start + BATCH_SIZE, icu_rad_notes_df.shape[0])\n",
    "    batch_df = icu_rad_notes_df.iloc[index_start:index_end]\n",
    "\n",
    "    for index, row in batch_df.iterrows():\n",
    "        curr_subject_id = int(row['subject_id'])\n",
    "        curr_note_id = row['note_id_x']\n",
    "        curr_text = row['text']\n",
    "        curr_dis_text = row['dis_text']\n",
    "\n",
    "        # Process 'text' column with BioBERT embeddings (with chunking)\n",
    "        text_chunk_parse, text_chunk_length = split_note_document(curr_text, 15)\n",
    "        text_embeddings = []\n",
    "        for chunk_batch_start in range(0, len(text_chunk_parse), BATCH_SIZE):\n",
    "            chunk_batch_end = min(chunk_batch_start + BATCH_SIZE, len(text_chunk_parse))\n",
    "            chunk_batch = text_chunk_parse[chunk_batch_start:chunk_batch_end]\n",
    "            embeddings_batch = process_batch(chunk_batch, device, model_name='biobert')\n",
    "            text_embeddings.extend(embeddings_batch)\n",
    "        \n",
    "        # Process 'dis_text' column with Longformer embeddings (no chunking)\n",
    "        dis_text_embeddings = process_batch([curr_dis_text], device, model_name='longformer')\n",
    "\n",
    "        # Store the results in the DataFrame\n",
    "        icu_rad_notes_df.at[index, 'biobert_embeddings'] = text_embeddings\n",
    "        icu_rad_notes_df.at[index, 'longformer_embeddings'] = dis_text_embeddings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_rad_notes_df.to_pickle(os.path.join(output_dir, \"DuoAllchunk_icu_notes_text_embeddings.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
