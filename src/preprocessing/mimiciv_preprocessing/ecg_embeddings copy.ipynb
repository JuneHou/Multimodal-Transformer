{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_path = \"/data/wang/junh/datasets/physionet.org/files/mimiciv/2.2\"\n",
    "mm_dir = \"/data/wang/junh/datasets/multimodal\"\n",
    "\n",
    "output_dir = os.path.join(mm_dir, \"preprocessing\")\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = os.path.join(mimic_iv_path, \"hosp\", \"admissions.csv.gz\")\n",
    "admissions_df = pd.read_csv(f_path, low_memory=False)\n",
    "admissions_df['admittime'] = pd.to_datetime(admissions_df['admittime'])\n",
    "admissions_df['dischtime'] = pd.to_datetime(admissions_df['dischtime'])\n",
    "\n",
    "icustays_df = pd.read_csv(os.path.join(mimic_iv_path, \"icu\", \"icustays.csv.gz\"), low_memory=False)\n",
    "icustays_df['intime'] = pd.to_datetime(icustays_df['intime'])\n",
    "icustays_df['outtime'] = pd.to_datetime(icustays_df['outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stays in ICU:  73181\n"
     ]
    }
   ],
   "source": [
    " print(\"Number of unique stays in ICU: \", icustays_df['stay_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_folder = '/data/wang/junh/datasets/physionet.org/files/mimic-iv-ecg/'\n",
    "\n",
    "records_list_df = pd.read_csv(os.path.join(ecg_folder, 'record_list.csv'))\n",
    "records_list_df['ecg_time'] = pd.to_datetime(records_list_df['ecg_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'study_id', 'file_name', 'ecg_time', 'path'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_list_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800035, 5)\n"
     ]
    }
   ],
   "source": [
    "print(records_list_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73181/73181 [06:19<00:00, 193.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calc_time_delta_hrs(start_time, end_time):\n",
    "    return (end_time - start_time).total_seconds() / 3600\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each ICU stay\n",
    "for index, row in tqdm(icustays_df.iterrows(), total=icustays_df.shape[0]):\n",
    "    curr_subject_no = row['subject_id']\n",
    "    curr_hadm_id = row['hadm_id']\n",
    "    curr_stay_id = row['stay_id']\n",
    "    curr_intime = row['intime']\n",
    "    curr_outtime = row['outtime']\n",
    "\n",
    "    # Filter ECG records for the current subject within the ICU stay timeframe\n",
    "    curr_subject_ecg = records_list_df[(records_list_df['subject_id'] == curr_subject_no) &\n",
    "                                       (records_list_df['ecg_time'] >= curr_intime) &\n",
    "                                       (records_list_df['ecg_time'] <= curr_outtime)]\n",
    "\n",
    "    if not curr_subject_ecg.empty:\n",
    "        # Get admissions data for the current subject\n",
    "        curr_admissions = admissions_df[admissions_df['subject_id'] == curr_subject_no]\n",
    "\n",
    "        for ecg_index, ecg_row in curr_subject_ecg.iterrows():\n",
    "            # Initialize the hospital time delta\n",
    "            hosp_time_delta = None\n",
    "\n",
    "            # Check for matching admission periods\n",
    "            matching_admissions = curr_admissions[(curr_admissions['admittime'] <= ecg_row['ecg_time']) &\n",
    "                                                  (curr_admissions['dischtime'] >= ecg_row['ecg_time'])]\n",
    "            if not matching_admissions.empty:\n",
    "                # Assuming one match, more complex logic needed if multiple matches are possible\n",
    "                adm_row = matching_admissions.iloc[0]\n",
    "                hosp_time_delta = calc_time_delta_hrs(adm_row['admittime'], ecg_row['ecg_time'])\n",
    "\n",
    "            # Create the output dictionary\n",
    "            tmp_dict = {\n",
    "                'subject_id': curr_subject_no,\n",
    "                'hadm_id': curr_hadm_id,\n",
    "                'stay_id': curr_stay_id,\n",
    "                'icu_time_delta': calc_time_delta_hrs(curr_intime, ecg_row['ecg_time']),\n",
    "                'hosp_time_delta': hosp_time_delta,\n",
    "                'ecg_time': ecg_row['ecg_time'],\n",
    "                'path': ecg_row['path']\n",
    "            }\n",
    "\n",
    "            # Append to the DataFrame\n",
    "            tmp_df = pd.DataFrame(tmp_dict, index=[0])\n",
    "            out_df = pd.concat([out_df, tmp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72167, 7)\n",
      "   subject_id   hadm_id   stay_id  icu_time_delta hosp_time_delta  \\\n",
      "0    10000980  26913865  39765666        1.700000        2.766667   \n",
      "1    10000980  26913865  39765666        5.900000        6.966667   \n",
      "2    10000980  26913865  39765666        6.216667        7.283333   \n",
      "3    10001884  26184834  37510196        2.948611       82.633333   \n",
      "4    10001884  26184834  37510196       28.781944      108.466667   \n",
      "\n",
      "             ecg_time                                      path  \n",
      "0 2189-06-27 10:24:00  files/p1000/p10000980/s49144190/49144190  \n",
      "1 2189-06-27 14:36:00  files/p1000/p10000980/s42742896/42742896  \n",
      "2 2189-06-27 14:55:00  files/p1000/p10000980/s41366957/41366957  \n",
      "3 2131-01-11 07:17:00  files/p1000/p10001884/s48806372/48806372  \n",
      "4 2131-01-12 09:07:00  files/p1000/p10001884/s43120472/43120472  \n"
     ]
    }
   ],
   "source": [
    "print(out_df.shape)\n",
    "print(out_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stay in out df:  35925\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique stay in out df: \", out_df['stay_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wfdb\n",
    "\n",
    "# f_path = '/data/wang/junh/githubs/Multimodal-Transformer/attia_encoder_256.keras'\n",
    "# encoder = tf.keras.models.load_model(f_path)\n",
    "\n",
    "# def load_ecg(path, stop_index=4096):\n",
    "#     rd_record = wfdb.rdrecord(path) \n",
    "#     sig = rd_record.p_signal\n",
    "#     sig = sig[:stop_index, :]\n",
    "#     return sig\n",
    "\n",
    "# out_df['embeddings'] = None\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# for index, row in tqdm(out_df.iterrows(), total=out_df.shape[0]):\n",
    "#     curr_ecg_path = os.path.join(ecg_folder, row['path'])\n",
    "#     wf = load_ecg(curr_ecg_path)\n",
    "#     out_df.at[index, 'embeddings'] = encoder.predict(wf.reshape(1, -1, 12), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights successfully loaded for layer: conv1d\n",
      "Weights successfully loaded for layer: batch_normalization\n",
      "Weights successfully loaded for layer: spatial_dropout1d\n",
      "Weights successfully loaded for layer: max_pooling1d\n",
      "Weights successfully loaded for layer: conv1d_1\n",
      "Weights successfully loaded for layer: batch_normalization_1\n",
      "Weights successfully loaded for layer: spatial_dropout1d_1\n",
      "Weights successfully loaded for layer: max_pooling1d_1\n",
      "Weights successfully loaded for layer: conv1d_2\n",
      "Weights successfully loaded for layer: batch_normalization_2\n",
      "Weights successfully loaded for layer: spatial_dropout1d_2\n",
      "Weights successfully loaded for layer: max_pooling1d_2\n",
      "Weights successfully loaded for layer: conv1d_3\n",
      "Weights successfully loaded for layer: batch_normalization_3\n",
      "Weights successfully loaded for layer: spatial_dropout1d_3\n",
      "Weights successfully loaded for layer: max_pooling1d_3\n",
      "Weights successfully loaded for layer: conv1d_4\n",
      "Weights successfully loaded for layer: batch_normalization_4\n",
      "Weights successfully loaded for layer: spatial_dropout1d_4\n",
      "Weights successfully loaded for layer: max_pooling1d_4\n",
      "Weights successfully loaded for layer: conv1d_5\n",
      "Weights successfully loaded for layer: batch_normalization_5\n",
      "Weights successfully loaded for layer: spatial_dropout1d_5\n",
      "Weights successfully loaded for layer: max_pooling1d_5\n",
      "Weights successfully loaded for layer: flatten\n",
      "Weights successfully loaded for layer: dense\n",
      "Weights successfully loaded for layer: dropout\n",
      "All weights loaded successfully into the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72167/72167 [08:40<00:00, 138.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# Load the model architecture from JSON file\n",
    "model_architecture = '/data/wang/junh/githubs/Multimodal-Transformer/attia_encoder_256/config.json'\n",
    "with open(model_architecture, 'r') as json_file:\n",
    "    architecture = json.load(json_file)\n",
    "    architecture_str = json.dumps(architecture)\n",
    "    model = model_from_json(architecture_str)\n",
    "\n",
    "# Load weights manually from the HDF5 file\n",
    "weights_path = '/data/wang/junh/githubs/Multimodal-Transformer/attia_encoder_256/model.weights.h5'\n",
    "with h5py.File(weights_path, 'r') as f:\n",
    "    for layer in model.layers:\n",
    "        layer_group = f['layers'].get(layer.name)\n",
    "        if layer_group and 'vars' in layer_group:\n",
    "            # Navigate to the 'vars' subgroup\n",
    "            vars_group = layer_group['vars']\n",
    "            # Collect weights assuming they are stored in the correct order under numbered keys\n",
    "            layer_weights = [vars_group[str(i)][:] for i in range(len(vars_group))]\n",
    "            layer.set_weights(layer_weights)\n",
    "            print(f\"Weights successfully loaded for layer: {layer.name}\")\n",
    "\n",
    "print(\"All weights loaded successfully into the model.\")\n",
    "\n",
    "# Function to load ECG\n",
    "def load_ecg(path, stop_index=4096):\n",
    "    rd_record = wfdb.rdrecord(path) \n",
    "    sig = rd_record.p_signal\n",
    "    sig = sig[:stop_index, :]\n",
    "    return sig\n",
    "\n",
    "# Prepare for batch processing\n",
    "batch_size = 32  # You can adjust the batch size depending on your GPU memory\n",
    "ecg_batch = []\n",
    "batch_indices = []\n",
    "out_df['embeddings'] = None\n",
    "\n",
    "# Process in batches\n",
    "for index, row in tqdm(out_df.iterrows(), total=out_df.shape[0]):\n",
    "    curr_ecg_path = os.path.join(ecg_folder, row['path'])\n",
    "    wf = load_ecg(curr_ecg_path)\n",
    "    ecg_batch.append(wf.reshape(1, -1, 12))\n",
    "    batch_indices.append(index)\n",
    "\n",
    "    # When batch is full, process it\n",
    "    if len(ecg_batch) == batch_size:\n",
    "        batch_ecgs = np.vstack(ecg_batch)\n",
    "        embeddings = model.predict(batch_ecgs, verbose=0)\n",
    "\n",
    "        # Assign embeddings to the correct rows\n",
    "        for i, idx in enumerate(batch_indices):\n",
    "            out_df.at[idx, 'embeddings'] = embeddings[i]\n",
    "\n",
    "        # Reset for next batch\n",
    "        ecg_batch = []\n",
    "        batch_indices = []\n",
    "\n",
    "# Process any remaining ECGs\n",
    "if ecg_batch:\n",
    "    batch_ecgs = np.vstack(ecg_batch)\n",
    "    embeddings = model.predict(batch_ecgs, verbose=0)\n",
    "    for i, idx in enumerate(batch_indices):\n",
    "        out_df.at[idx, 'embeddings'] = embeddings[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_dir = \"/data/wang/junh/datasets/multimodal\"\n",
    "output_dir = os.path.join(mm_dir, \"preprocessing\")\n",
    "\n",
    "out_df.to_pickle(os.path.join(output_dir, \"ecg_embeddings_hosp.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
