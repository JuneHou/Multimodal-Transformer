{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from npj_utils import *\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iv_path = \"/data/wang/junh/datasets/physionet.org/files/mimiciv/2.2\"\n",
    "mm_dir = \"/data/wang/junh/datasets/multimodal\"\n",
    "\n",
    "output_dir = os.path.join(mm_dir, \"preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "base_name = \"pheno\" # ihm, los, pheno\n",
    "\n",
    "if \"pheno\" in base_name:\n",
    "    base_name += \"-all\"\n",
    "else:\n",
    "    base_name += \"-48\"\n",
    "\n",
    "# base_name += \"-cxr-notes-ecg-missingInd\"\n",
    "base_name += \"-cxr-notes-missingInd\"\n",
    "\n",
    "f_path = os.path.join(output_dir, f\"train_{base_name}_stays.pkl\")\n",
    "\n",
    "with open(f_path, \"rb\") as f:\n",
    "    train_stays = pickle.load(f)\n",
    "\n",
    "f_path = os.path.join(output_dir, f\"val_{base_name}_stays.pkl\")\n",
    "\n",
    "with open(f_path, \"rb\") as f:\n",
    "    val_stays = pickle.load(f)\n",
    "\n",
    "f_path = os.path.join(output_dir, f\"test_{base_name}_stays.pkl\")\n",
    "\n",
    "with open(f_path, \"rb\") as f:\n",
    "    test_stays = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_notes = True\n",
    "include_cxr = True\n",
    "include_ecg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Time Series Embeddings:   0%|          | 0/10178 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Time Series Embeddings: 100%|██████████| 10178/10178 [01:07<00:00, 150.19it/s]\n",
      "Calculating Text Embeddings: 100%|██████████| 10178/10178 [10:11<00:00, 16.65it/s]\n",
      "Calculating CXR Embeddings: 100%|██████████| 10178/10178 [15:07<00:00, 11.21it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = calc_ts_embeddings(train_stays)\n",
    "\n",
    "if include_notes:\n",
    "    txt_df = calc_avg_text_embedding(train_stays)\n",
    "    X_train = pd.concat([X_train, txt_df], axis=1)\n",
    "\n",
    "if include_cxr:\n",
    "    cxr_df = calc_avg_cxr_embedding(train_stays) \n",
    "    X_train = pd.concat([X_train, cxr_df], axis=1)\n",
    "\n",
    "if include_ecg:\n",
    "    ecg_df = calc_avg_ecg_embedding(train_stays)\n",
    "    X_train = pd.concat([X_train, ecg_df], axis=1)\n",
    "\n",
    "y_train = extract_labels(train_stays)\n",
    "\n",
    "col_names = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Time Series Embeddings:   0%|          | 0/2181 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Time Series Embeddings: 100%|██████████| 2181/2181 [00:15<00:00, 141.88it/s]\n",
      "Calculating Text Embeddings: 100%|██████████| 2181/2181 [00:20<00:00, 107.40it/s]\n",
      "Calculating CXR Embeddings: 100%|██████████| 2181/2181 [00:18<00:00, 116.11it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = calc_ts_embeddings(test_stays)\n",
    "\n",
    "if include_notes:\n",
    "    txt_df = calc_avg_text_embedding(test_stays)\n",
    "    X_test = pd.concat([X_test, txt_df], axis=1)\n",
    "\n",
    "if include_cxr:\n",
    "    cxr_df = calc_avg_cxr_embedding(test_stays)\n",
    "    X_test = pd.concat([X_test, cxr_df], axis=1)\n",
    "\n",
    "if include_ecg:\n",
    "    ecg_df = calc_avg_ecg_embedding(test_stays)\n",
    "    X_test = pd.concat([X_test, ecg_df], axis=1)\n",
    "\n",
    "y_test = extract_labels(test_stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Time Series Embeddings: 100%|██████████| 2181/2181 [00:16<00:00, 133.80it/s]\n",
      "Calculating Text Embeddings: 100%|██████████| 2181/2181 [00:14<00:00, 155.28it/s]\n",
      "Calculating CXR Embeddings: 100%|██████████| 2181/2181 [00:20<00:00, 107.54it/s]\n"
     ]
    }
   ],
   "source": [
    "X_val = calc_ts_embeddings(val_stays)\n",
    "\n",
    "if include_notes:\n",
    "    txt_df = calc_avg_text_embedding(val_stays)\n",
    "    X_val = pd.concat([X_val, txt_df], axis=1)\n",
    "\n",
    "if include_cxr:\n",
    "    cxr_df = calc_avg_cxr_embedding(val_stays)\n",
    "    X_val = pd.concat([X_val, cxr_df], axis=1)\n",
    "\n",
    "if include_ecg:\n",
    "    ecg_df = calc_avg_ecg_embedding(val_stays)\n",
    "    X_val = pd.concat([X_val, ecg_df], axis=1)\n",
    "\n",
    "y_val = extract_labels(val_stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_util import run_xgb, run_xgb_multilabel\n",
    "\n",
    "seed = 1\n",
    "if \"pheno\" in base_name:\n",
    "    y_pred_test, y_pred_prob_test, y_pred_train, y_pred_prob_train, gs = run_xgb_multilabel(X_train, y_train, X_test, gpu=0, seed=seed, n_jobs=16)\n",
    "else:\n",
    "    y_pred_test, y_pred_prob_test, y_pred_train, y_pred_prob_train, gs = run_xgb(X_train, y_train, X_test, gpu=0, seed=seed, n_jobs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pheno (no ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "(2181, 2)\n",
      "(25,)\n",
      "(10178, 2)\n",
      "{'estimator__learning_rate': 0.3, 'estimator__max_depth': 5, 'estimator__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_test[0].shape)\n",
    "print(y_pred_prob_test[0].shape)\n",
    "print(y_pred_train[0].shape)\n",
    "print(y_pred_prob_train[0].shape)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating label 1\n",
      "Accuracy for label 1: 0.8055937643282898\n",
      "AUC for label 1: 0.8316972298088178\n",
      "F1 Score for label 1: 0.5664621676891616\n",
      "Precision for label 1: 0.647196261682243\n",
      "Recall for label 1: 0.5036363636363637\n",
      "Evaluating label 2\n",
      "Accuracy for label 2: 0.7725813846859239\n",
      "AUC for label 2: 0.6207606993238511\n",
      "F1 Score for label 2: 0.11743772241992882\n",
      "Precision for label 2: 0.308411214953271\n",
      "Recall for label 2: 0.07252747252747253\n",
      "Evaluating label 3\n",
      "Accuracy for label 3: 0.8702430077945896\n",
      "AUC for label 3: 0.7248408583500933\n",
      "F1 Score for label 3: 0.22465753424657536\n",
      "Precision for label 3: 0.5189873417721519\n",
      "Recall for label 3: 0.14335664335664336\n",
      "Evaluating label 4\n",
      "Accuracy for label 4: 0.619440623567171\n",
      "AUC for label 4: 0.6549821357681619\n",
      "F1 Score for label 4: 0.5053635280095352\n",
      "Precision for label 4: 0.5564304461942258\n",
      "Recall for label 4: 0.462882096069869\n",
      "Evaluating label 5\n",
      "Accuracy for label 5: 0.6790463090325539\n",
      "AUC for label 5: 0.7518983509783971\n",
      "F1 Score for label 5: 0.6741154562383612\n",
      "Precision for label 5: 0.684957426679281\n",
      "Recall for label 5: 0.6636113657195234\n",
      "Evaluating label 6\n",
      "Accuracy for label 6: 0.6414488766620816\n",
      "AUC for label 6: 0.686177805182152\n",
      "F1 Score for label 6: 0.5754614549402823\n",
      "Precision for label 6: 0.6064073226544623\n",
      "Recall for label 6: 0.5475206611570248\n",
      "Evaluating label 7\n",
      "Accuracy for label 7: 0.7725813846859239\n",
      "AUC for label 7: 0.831497716435861\n",
      "F1 Score for label 7: 0.6265060240963856\n",
      "Precision for label 7: 0.659270998415214\n",
      "Recall for label 7: 0.5968436154949784\n",
      "Evaluating label 8\n",
      "Accuracy for label 8: 0.9142595139844109\n",
      "AUC for label 8: 0.7414305146225346\n",
      "F1 Score for label 8: 0.0878048780487805\n",
      "Precision for label 8: 0.3103448275862069\n",
      "Recall for label 8: 0.05113636363636364\n",
      "Evaluating label 9\n",
      "Accuracy for label 9: 0.7235213204951857\n",
      "AUC for label 9: 0.738798377443809\n",
      "F1 Score for label 9: 0.5202863961813843\n",
      "Precision for label 9: 0.6100746268656716\n",
      "Recall for label 9: 0.4535367545076283\n",
      "Evaluating label 10\n",
      "Accuracy for label 10: 0.8885832187070152\n",
      "AUC for label 10: 0.7256182428501868\n",
      "F1 Score for label 10: 0.13523131672597866\n",
      "Precision for label 10: 0.4523809523809524\n",
      "Recall for label 10: 0.0794979079497908\n",
      "Evaluating label 11\n",
      "Accuracy for label 11: 0.6483264557542412\n",
      "AUC for label 11: 0.6804640644113166\n",
      "F1 Score for label 11: 0.5431804645622395\n",
      "Precision for label 11: 0.5728643216080402\n",
      "Recall for label 11: 0.5164212910532276\n",
      "Evaluating label 12\n",
      "Accuracy for label 12: 0.7817514901421366\n",
      "AUC for label 12: 0.8086436029954646\n",
      "F1 Score for label 12: 0.6148867313915858\n",
      "Precision for label 12: 0.6884057971014492\n",
      "Recall for label 12: 0.5555555555555556\n",
      "Evaluating label 13\n",
      "Accuracy for label 13: 0.9073819348922513\n",
      "AUC for label 13: 0.8960532973519986\n",
      "F1 Score for label 13: 0.5190476190476191\n",
      "Precision for label 13: 0.6228571428571429\n",
      "Recall for label 13: 0.4448979591836735\n",
      "Evaluating label 14\n",
      "Accuracy for label 14: 0.782668500687758\n",
      "AUC for label 14: 0.7163643756622884\n",
      "F1 Score for label 14: 0.315028901734104\n",
      "Precision for label 14: 0.5046296296296297\n",
      "Recall for label 14: 0.22899159663865545\n",
      "Evaluating label 15\n",
      "Accuracy for label 15: 0.8363136176066025\n",
      "AUC for label 15: 0.6902151211361738\n",
      "F1 Score for label 15: 0.2119205298013245\n",
      "Precision for label 15: 0.5\n",
      "Recall for label 15: 0.13445378151260504\n",
      "Evaluating label 16\n",
      "Accuracy for label 16: 0.8477762494268684\n",
      "AUC for label 16: 0.6467444553308986\n",
      "F1 Score for label 16: 0.18226600985221675\n",
      "Precision for label 16: 0.46835443037974683\n",
      "Recall for label 16: 0.11314984709480122\n",
      "Evaluating label 17\n",
      "Accuracy for label 17: 0.7757909215955984\n",
      "AUC for label 17: 0.8390191106438245\n",
      "F1 Score for label 17: 0.6949469744229569\n",
      "Precision for label 17: 0.7406914893617021\n",
      "Recall for label 17: 0.654524089306698\n",
      "Evaluating label 18\n",
      "Accuracy for label 18: 0.8661164603392939\n",
      "AUC for label 18: 0.5557960307973302\n",
      "F1 Score for label 18: 0.0457516339869281\n",
      "Precision for label 18: 0.3181818181818182\n",
      "Recall for label 18: 0.02464788732394366\n",
      "Evaluating label 19\n",
      "Accuracy for label 19: 0.9234296194406235\n",
      "AUC for label 19: 0.6629948045522018\n",
      "F1 Score for label 19: 0.011834319526627219\n",
      "Precision for label 19: 0.1111111111111111\n",
      "Recall for label 19: 0.00625\n",
      "Evaluating label 20\n",
      "Accuracy for label 20: 0.8285190279688216\n",
      "AUC for label 20: 0.7615078986934333\n",
      "F1 Score for label 20: 0.42813455657492355\n",
      "Precision for label 20: 0.6306306306306306\n",
      "Recall for label 20: 0.32407407407407407\n",
      "Evaluating label 21\n",
      "Accuracy for label 21: 0.9188445667125172\n",
      "AUC for label 21: 0.7940824279898108\n",
      "F1 Score for label 21: 0.24034334763948498\n",
      "Precision for label 21: 0.5714285714285714\n",
      "Recall for label 21: 0.15217391304347827\n",
      "Evaluating label 22\n",
      "Accuracy for label 22: 0.8120128381476387\n",
      "AUC for label 22: 0.872829558520249\n",
      "F1 Score for label 22: 0.7281167108753315\n",
      "Precision for label 22: 0.7593360995850622\n",
      "Recall for label 22: 0.6993630573248407\n",
      "Evaluating label 23\n",
      "Accuracy for label 23: 0.8528198074277854\n",
      "AUC for label 23: 0.8991466614413319\n",
      "F1 Score for label 23: 0.6638743455497382\n",
      "Precision for label 23: 0.6921397379912664\n",
      "Recall for label 23: 0.6378269617706237\n",
      "Evaluating label 24\n",
      "Accuracy for label 24: 0.7482806052269602\n",
      "AUC for label 24: 0.6755680564024223\n",
      "F1 Score for label 24: 0.37113402061855666\n",
      "Precision for label 24: 0.5294117647058824\n",
      "Recall for label 24: 0.2857142857142857\n",
      "Evaluating label 25\n",
      "Accuracy for label 25: 0.8266850068775791\n",
      "AUC for label 25: 0.8528626813674912\n",
      "F1 Score for label 25: 0.544578313253012\n",
      "Precision for label 25: 0.6827794561933535\n",
      "Recall for label 25: 0.4529058116232465\n",
      "Macro F1 Score: 0.4059348382973209\n",
      "Micro F1 Score: 0.5230131062177309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Evaluate for each label\n",
    "# Assuming y_test is of shape (n_samples, n_labels)\n",
    "for i in range(y_test.shape[1]):  # Iterate over each label\n",
    "    print(f\"Evaluating label {i+1}\")\n",
    "    \n",
    "    y_test_label = y_test[:, i]  # Extract the true labels for label i\n",
    "    y_pred_label = y_pred_test[:, i]  # Extract the predicted labels for label i\n",
    "    y_pred_prob_label = y_pred_prob_test[i][:, 1]  # Extract the probability for the positive class (1) for label i\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy = accuracy_score(y_test_label, y_pred_label)\n",
    "    auc = roc_auc_score(y_test_label, y_pred_prob_label)  # Check shape of y_pred_prob_label\n",
    "    f1 = f1_score(y_test_label, y_pred_label)\n",
    "    precision = precision_score(y_test_label, y_pred_label)\n",
    "    recall = recall_score(y_test_label, y_pred_label)\n",
    "\n",
    "    print(f\"Accuracy for label {i+1}: {accuracy}\")\n",
    "    print(f\"AUC for label {i+1}: {auc}\")\n",
    "    print(f\"F1 Score for label {i+1}: {f1}\")\n",
    "    print(f\"Precision for label {i+1}: {precision}\")\n",
    "    print(f\"Recall for label {i+1}: {recall}\")\n",
    "\n",
    "# Optionally, aggregate metrics across all labels (macro or micro averaging)\n",
    "macro_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "micro_f1 = f1_score(y_test, y_pred_test, average='micro')\n",
    "print(f\"Macro F1 Score: {macro_f1}\")\n",
    "print(f\"Micro F1 Score: {micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (micro): 0.6809527390950785\n",
      "AUC (macro): 0.6277059636191837\n",
      "AUC (weighted): 0.6520082985302481\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUC (micro)\n",
    "avg_auc_micro = roc_auc_score(y_test, y_pred_test, average='micro')\n",
    "\n",
    "# Calculate AUC (macro)\n",
    "avg_auc_macro = roc_auc_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "# Calculate AUC (weighted)\n",
    "avg_auc_weighted = roc_auc_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'AUC (micro): {avg_auc_micro}')\n",
    "print(f'AUC (macro): {avg_auc_macro}')\n",
    "print(f'AUC (weighted): {avg_auc_weighted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = MultiOutputClassifier(xgb.XGBClassifier(verbosity=2, seed=42,\n",
    "                                                  tree_method='gpu_hist', gpu_id=1,\n",
    "                                                  eval_metric='logloss', n_jobs=32,learning_rate= 0.3, max_depth= 5, n_estimators= 200))\n",
    "est.fit(X_train, y_train)\n",
    "y_pred_prob_test = est.predict_proba(X_test)\n",
    "y_pred_test = est.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.4059348382973209\n",
      "Micro F1 Score: 0.5230131062177309\n"
     ]
    }
   ],
   "source": [
    "# Optionally, aggregate metrics across all labels (macro or micro averaging)\n",
    "macro_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "micro_f1 = f1_score(y_test, y_pred_test, average='micro')\n",
    "print(f\"Macro F1 Score: {macro_f1}\")\n",
    "print(f\"Micro F1 Score: {micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (micro): 0.6809527390950785\n",
      "AUC (macro): 0.6277059636191837\n",
      "AUC (weighted): 0.6520082985302481\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUC (micro)\n",
    "avg_auc_micro = roc_auc_score(y_test, y_pred_test, average='micro')\n",
    "\n",
    "# Calculate AUC (macro)\n",
    "avg_auc_macro = roc_auc_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "# Calculate AUC (weighted)\n",
    "avg_auc_weighted = roc_auc_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'AUC (micro): {avg_auc_micro}')\n",
    "print(f'AUC (macro): {avg_auc_macro}')\n",
    "print(f'AUC (weighted): {avg_auc_weighted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOS (no ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5270,)\n",
      "(5270, 2)\n",
      "(24591,)\n",
      "(24591, 2)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_test.shape)\n",
    "print(y_pred_prob_test.shape)\n",
    "print(y_pred_train.shape)\n",
    "print(y_pred_prob_train.shape)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of outcome: 0.47324478178368123\n",
      "F1 Score: 0.7402229359531457\n",
      "Accuracy: 0.7390891840607211\n",
      "Balanced Accuracy: 0.741445752226092\n",
      "AUC: 0.8170199256313135\n",
      "AUPRC: 0.7746974031364304\n",
      "Confusion Matrix: [[1936  840]\n",
      " [ 535 1959]]\n"
     ]
    }
   ],
   "source": [
    "est = xgb.XGBClassifier(verbosity=2, seed=seed,eval_metric='logloss', n_jobs=32,learning_rate= 0.05, max_depth= 5, n_estimators= 300, device=\"cuda\")\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = est.predict(X_test)\n",
    "y_pred_prob_test = est.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "_ = evaluate_model(y_test, y_pred_test, y_pred_prob_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set is: 0.8371074939308314\n",
      "Accuracy for Training Set is: 0.8335569923955919\n",
      "Balanced Accuracy for Training Set is: 0.8356921438594688\n",
      "AUC for Training Set is: 0.9163461677710176\n",
      "AUPRC for Training Set is: 0.9007149201542283\n",
      "Confusion Matrix for Training Set is: \n",
      "[[ 9981  2796]\n",
      " [ 1297 10517]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the training set\n",
    "y_pred_prob_train = est.predict_proba(X_train)\n",
    "y_pred_train = est.predict(X_train)\n",
    "\n",
    "# F1 Score for the training set\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "print(f\"F1 Score for Training Set is: {f1_train}\")\n",
    "\n",
    "# Accuracy for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Accuracy for Training Set is: {accuracy_train}\")\n",
    "\n",
    "# Balanced Accuracy for the training set\n",
    "balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Balanced Accuracy for Training Set is: {balanced_accuracy_train}\")\n",
    "\n",
    "# AUC for the training set\n",
    "# Ensure that y_pred_prob_train[:, 1] exists if it's a binary classification\n",
    "if len(np.unique(y_train)) == 2:\n",
    "    auc_train = roc_auc_score(y_train, y_pred_prob_train[:, 1])\n",
    "    print(f\"AUC for Training Set is: {auc_train}\")\n",
    "else:\n",
    "    print(\"AUC is typically used for binary classification. Adjust the code for multi-class if needed.\")\n",
    "\n",
    "# AUPRC for the training set\n",
    "if len(np.unique(y_train)) == 2:\n",
    "    auprc_train = average_precision_score(y_train, y_pred_prob_train[:, 1])\n",
    "    print(f\"AUPRC for Training Set is: {auprc_train}\")\n",
    "else:\n",
    "    print(\"AUPRC is typically used for binary classification. Adjust the code for multi-class if needed.\")\n",
    "\n",
    "# Confusion Matrix for the training set\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "print(f\"Confusion Matrix for Training Set is: \\n{cm_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Testing Set is: 0.7402229359531457\n",
      "Accuracy for Testing Set is: 0.7390891840607211\n",
      "Balanced Accuracy for Testing Set is: 0.741445752226092\n",
      "AUC for Testing Set is: 0.8170199256313135\n",
      "AUPRC for Testing Set is: 0.7746974031364304\n",
      "Confusion Matrix for Testing Set is: \n",
      "[[1936  840]\n",
      " [ 535 1959]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "# Assumed correction: Ensure y_pred_prob_test is fetched correctly and has two columns\n",
    "# y_pred_prob_test = model.predict_proba(X_test)  # Correctly fetching test probabilities\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "print(f\"F1 Score for Testing Set is: {f1}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy for Testing Set is: {accuracy}\")\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Balanced Accuracy for Testing Set is: {balanced_accuracy}\")\n",
    "\n",
    "# AUC\n",
    "if y_pred_prob_test.ndim == 2 and y_pred_prob_test.shape[1] == 2:  # Ensuring binary classification with two probability outputs\n",
    "    auc = roc_auc_score(y_test, y_pred_prob_test[:, 1])\n",
    "    print(f\"AUC for Testing Set is: {auc}\")\n",
    "else:\n",
    "    print(\"Error: Expected binary class probabilities for AUC calculation.\")\n",
    "\n",
    "# AUPRC\n",
    "if y_pred_prob_test.ndim == 2 and y_pred_prob_test.shape[1] == 2:\n",
    "    auprc = average_precision_score(y_test, y_pred_prob_test[:, 1])\n",
    "    print(f\"AUPRC for Testing Set is: {auprc}\")\n",
    "else:\n",
    "    print(\"Error: Expected binary class probabilities for AUPRC calculation.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"Confusion Matrix for Testing Set is: \\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHM (no ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5270,)\n",
      "(5270,)\n",
      "(24591,)\n",
      "(24591,)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_test.shape)\n",
    "print(y_pred_prob_test.shape)\n",
    "print(y_pred_train.shape)\n",
    "print(y_pred_prob_train.shape)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of outcome: 0.14193548387096774\n",
      "F1 Score: 0.3940242763772176\n",
      "Accuracy: 0.8768500948766603\n",
      "Balanced Accuracy: 0.6286588798198706\n",
      "AUC: 0.8554489696244386\n",
      "AUPRC: 0.5165519978746538\n",
      "Confusion Matrix: [[4410  112]\n",
      " [ 537  211]]\n"
     ]
    }
   ],
   "source": [
    "est = xgb.XGBClassifier(verbosity=2, seed=seed,eval_metric='logloss', n_jobs=32,learning_rate= 0.05, max_depth= 5, n_estimators= 200, device=\"cuda\")\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = est.predict(X_test)\n",
    "y_pred_prob_test = est.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "_ = evaluate_model(y_test, y_pred_test, y_pred_prob_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set is: 0.6337464251668257\n",
      "Accuracy for Training Set is: 0.9218819893456955\n",
      "Balanced Accuracy for Training Set is: 0.7369427993874346\n",
      "AUC for Training Set is: 0.9487933938991525\n",
      "AUPRC for Training Set is: 0.8271620505190398\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the training set\n",
    "y_pred_prob_train = est.predict_proba(X_train)\n",
    "y_pred_train = est.predict(X_train)\n",
    "\n",
    "# F1 Score for the training set\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "print(f\"F1 Score for Training Set is: {f1_train}\")\n",
    "\n",
    "# Accuracy for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Accuracy for Training Set is: {accuracy_train}\")\n",
    "\n",
    "# Balanced Accuracy for the training set\n",
    "balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Balanced Accuracy for Training Set is: {balanced_accuracy_train}\")\n",
    "\n",
    "# AUC for the training set\n",
    "# Ensure that y_pred_prob_train[:, 1] exists if it's a binary classification\n",
    "if len(np.unique(y_train)) == 2:\n",
    "    auc_train = roc_auc_score(y_train, y_pred_prob_train[:, 1])\n",
    "    print(f\"AUC for Training Set is: {auc_train}\")\n",
    "else:\n",
    "    print(\"AUC is typically used for binary classification. Adjust the code for multi-class if needed.\")\n",
    "\n",
    "# AUPRC for the training set\n",
    "if len(np.unique(y_train)) == 2:\n",
    "    auprc_train = average_precision_score(y_train, y_pred_prob_train[:, 1])\n",
    "    print(f\"AUPRC for Training Set is: {auprc_train}\")\n",
    "else:\n",
    "    print(\"AUPRC is typically used for binary classification. Adjust the code for multi-class if needed.\")\n",
    "\n",
    "# Confusion Matrix for the training set\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Testing Set is: 0.3940242763772176\n",
      "Accuracy for Testing Set is: 0.8768500948766603\n",
      "Balanced Accuracy for Testing Set is: 0.6286588798198706\n",
      "AUC for Testing Set is: 0.8554489696244386\n",
      "AUPRC for Testing Set is: 0.5165519978746538\n",
      "Confusion Matrix for Testing Set is: \n",
      "[[4410  112]\n",
      " [ 537  211]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "# Assumed correction: Ensure y_pred_prob_test is fetched correctly and has two columns\n",
    "# y_pred_prob_test = model.predict_proba(X_test)  # Correctly fetching test probabilities\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "print(f\"F1 Score for Testing Set is: {f1}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy for Testing Set is: {accuracy}\")\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Balanced Accuracy for Testing Set is: {balanced_accuracy}\")\n",
    "\n",
    "# AUC\n",
    "if y_pred_prob_test.ndim == 2 and y_pred_prob_test.shape[1] == 2:  # Ensuring binary classification with two probability outputs\n",
    "    auc = roc_auc_score(y_test, y_pred_prob_test[:, 1])\n",
    "    print(f\"AUC for Testing Set is: {auc}\")\n",
    "else:\n",
    "    print(\"Error: Expected binary class probabilities for AUC calculation.\")\n",
    "\n",
    "# AUPRC\n",
    "if y_pred_prob_test.ndim == 2 and y_pred_prob_test.shape[1] == 2:\n",
    "    auprc = average_precision_score(y_test, y_pred_prob_test[:, 1])\n",
    "    print(f\"AUPRC for Testing Set is: {auprc}\")\n",
    "else:\n",
    "    print(\"Error: Expected binary class probabilities for AUPRC calculation.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"Confusion Matrix for Testing Set is: \\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of outcome: 0.14193548387096774\n",
      "F1 Score: 0.3940242763772176\n",
      "Accuracy: 0.8768500948766603\n",
      "Balanced Accuracy: 0.6286588798198706\n",
      "AUC: 0.8554489696244386\n",
      "AUPRC: 0.5165519978746538\n",
      "Confusion Matrix: [[4410  112]\n",
      " [ 537  211]]\n"
     ]
    }
   ],
   "source": [
    "est = xgb.XGBClassifier(verbosity=2, seed=seed,eval_metric='logloss', n_jobs=32,learning_rate= 0.05, max_depth= 5, n_estimators= 200, device=\"cuda\")\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = est.predict(X_test)\n",
    "y_pred_prob_test = est.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "_ = evaluate_model(y_test, y_pred_test, y_pred_prob_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set is: 0.6337464251668257\n",
      "Accuracy for Training Set is: 0.9218819893456955\n",
      "Balanced Accuracy for Training Set is: 0.7369427993874346\n",
      "AUC for Training Set is: 0.9487933938991525\n",
      "AUPRC for Training Set is: 0.8271620505190398\n",
      "Confusion Matrix for Training Set is: \n",
      "[[21008   116]\n",
      " [ 1805  1662]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the training set\n",
    "y_pred_prob_train = est.predict_proba(X_train)\n",
    "y_pred_train = est.predict(X_train)\n",
    "\n",
    "# F1 Score for the training set\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "print(f\"F1 Score for Training Set is: {f1_train}\")\n",
    "\n",
    "# Accuracy for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Accuracy for Training Set is: {accuracy_train}\")\n",
    "\n",
    "# Balanced Accuracy for the training set\n",
    "balanced_accuracy_train = balanced_accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Balanced Accuracy for Training Set is: {balanced_accuracy_train}\")\n",
    "\n",
    "# AUC for the training set\n",
    "# Ensure that y_pred_prob_train[:, 1] exists if it's a binary classification\n",
    "if len(np.unique(y_train)) == 2:\n",
    "    auc_train = roc_auc_score(y_train, y_pred_prob_train[:, 1])\n",
    "    print(f\"AUC for Training Set is: {auc_train}\")\n",
    "else:\n",
    "    print(\"AUC is typically used for binary classification. Adjust the code for multi-class if needed.\")\n",
    "\n",
    "# AUPRC for the training set\n",
    "if len(np.unique(y_train)) == 2:\n",
    "    auprc_train = average_precision_score(y_train, y_pred_prob_train[:, 1])\n",
    "    print(f\"AUPRC for Training Set is: {auprc_train}\")\n",
    "else:\n",
    "    print(\"AUPRC is typically used for binary classification. Adjust the code for multi-class if needed.\")\n",
    "\n",
    "# Confusion Matrix for the training set\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "print(f\"Confusion Matrix for Training Set is: \\n{cm_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Testing Set is: 0.3940242763772176\n",
      "Accuracy for Testing Set is: 0.8768500948766603\n",
      "Balanced Accuracy for Testing Set is: 0.6286588798198706\n",
      "AUC for Testing Set is: 0.8554489696244386\n",
      "AUPRC for Testing Set is: 0.5165519978746538\n",
      "Confusion Matrix for Testing Set is: \n",
      "[[4410  112]\n",
      " [ 537  211]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "# Assumed correction: Ensure y_pred_prob_test is fetched correctly and has two columns\n",
    "# y_pred_prob_test = model.predict_proba(X_test)  # Correctly fetching test probabilities\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "print(f\"F1 Score for Testing Set is: {f1}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy for Testing Set is: {accuracy}\")\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Balanced Accuracy for Testing Set is: {balanced_accuracy}\")\n",
    "\n",
    "# AUC\n",
    "if y_pred_prob_test.ndim == 2 and y_pred_prob_test.shape[1] == 2:  # Ensuring binary classification with two probability outputs\n",
    "    auc = roc_auc_score(y_test, y_pred_prob_test[:, 1])\n",
    "    print(f\"AUC for Testing Set is: {auc}\")\n",
    "else:\n",
    "    print(\"Error: Expected binary class probabilities for AUC calculation.\")\n",
    "\n",
    "# AUPRC\n",
    "if y_pred_prob_test.ndim == 2 and y_pred_prob_test.shape[1] == 2:\n",
    "    auprc = average_precision_score(y_test, y_pred_prob_test[:, 1])\n",
    "    print(f\"AUPRC for Testing Set is: {auprc}\")\n",
    "else:\n",
    "    print(\"Error: Expected binary class probabilities for AUPRC calculation.\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(f\"Confusion Matrix for Testing Set is: \\n{cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. Diastolic BP_max (0.058471)\n",
      "2. GCS - Eye Opening_max (0.014327)\n",
      "3. Respiratory Rate_max (0.009765)\n",
      "4. Vancomycin_mean (0.009507)\n",
      "5. Respiratory Rate_mean (0.008515)\n",
      "6. GCS - Eye Opening_min (0.007363)\n",
      "7. Vancomycin_min (0.006762)\n",
      "8. Respiratory Rate_min (0.006646)\n",
      "9. Absolute Neutrophil Count_mean (0.006516)\n",
      "10. te_233 (0.005733)\n",
      "11. Diastolic BP_mean (0.005556)\n",
      "12. Alkaline Phosphate_mean (0.005449)\n",
      "13. te_731 (0.005115)\n",
      "14. Diastolic BP_min (0.004965)\n",
      "15. Systolic BP_mean (0.004898)\n",
      "16. te_519 (0.004790)\n",
      "17. te_107 (0.004631)\n",
      "18. te_660 (0.004585)\n",
      "19. Diastolic BP_trend (0.004287)\n",
      "20. te_280 (0.004283)\n",
      "21. te_61 (0.004194)\n",
      "22. Vancomycin_max (0.004176)\n",
      "23. te_50 (0.004152)\n",
      "24. Urea Nitrogen_min (0.004097)\n",
      "25. Creatinine_max (0.003986)\n",
      "26. te_515 (0.003961)\n",
      "27. Temperature_sumabsdiff (0.003848)\n",
      "28. Creatinine_mean (0.003763)\n",
      "29. te_483 (0.003687)\n",
      "30. PH_variance (0.003676)\n",
      "31. Potassium_min (0.003675)\n",
      "32. te_555 (0.003536)\n",
      "33. Urea Nitrogen_mean (0.003460)\n",
      "34. te_487 (0.003370)\n",
      "35. Inspired O2 Fraction_max (0.003323)\n",
      "36. Creatinine_variance (0.003310)\n",
      "37. Platelet Count_mean (0.003276)\n",
      "38. te_1 (0.003250)\n",
      "39. Creatinine_min (0.003223)\n",
      "40. GCS - Eye Opening_mean (0.003169)\n",
      "41. te_643 (0.003122)\n",
      "42. GCS - Eye Opening_trend (0.003066)\n",
      "43. Diastolic BP_diff (0.003049)\n",
      "44. te_462 (0.002959)\n",
      "45. GCS - Motor Response_npeaks (0.002952)\n",
      "46. te_668 (0.002874)\n",
      "47. Diastolic BP_sumabsdiff (0.002824)\n",
      "48. Weight_min (0.002803)\n",
      "49. Absolute Neutrophil Count_min (0.002617)\n",
      "50. Calcium_maxdiff (0.002608)\n"
     ]
    }
   ],
   "source": [
    "est.feature_importances_\n",
    "\n",
    "# Get the top 10 most important features\n",
    "indices = np.argsort(est.feature_importances_)[::-1]\n",
    "top_indices = indices[:100]\n",
    "print('Feature ranking:')\n",
    "for i in range(50):\n",
    "    print('%d. %s (%f)' % (i + 1, col_names[top_indices[i]], est.feature_importances_[top_indices[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
